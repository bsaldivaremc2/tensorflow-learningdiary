{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network (DNN) with tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I show a DNN using tensorflow for matrix operations but tunned without it and a more pure tensorflow implementation. The first one takes more time of execution. In addition, if you have ever got a **Nan** value in the cost function it migth be because your sigmoid function is causing a 1.0, thus a zero by zero multiplication in the cost function when y==0. Here, I address this issue. See the **Notes** section for detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376, 2501)\n",
      "X shape (2376, 2500) Y shape (2376, 38) m 2376 n 2500 #Classes 38\n",
      "Classes: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0]\n"
     ]
    }
   ],
   "source": [
    "dataset_dir='/home/...Dir'\n",
    "dataset_file='/images.npy'\n",
    "dataset_location = dataset_dir+'/'+dataset_file\n",
    "dataset_np = np.load(dataset_location)\n",
    "print(dataset_np.shape)\n",
    "\n",
    "Xdata = dataset_np[:,:-1].astype(\"float32\") +0.5\n",
    "Xdata = Xdata/500.0\n",
    "Ydata = dataset_np[:,-1:].flatten()#.astype(\"float32\")\n",
    "targets=list(set(Ydata.tolist()))\n",
    "\n",
    "Ydata = pd.get_dummies(Ydata)\n",
    "Ydata = Ydata.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xdata,Ydata)\n",
    "\n",
    "#X shape here is m by n so we will transpose it : no pure tensorflow\n",
    "#Xdata = np.transpose(Xdata)\n",
    "#Ydata = np.transpose(Ydata)\n",
    "\n",
    "n=Xdata.shape[1]\n",
    "m=Xdata.shape[0]\n",
    "\n",
    "t = len(targets)\n",
    "\n",
    "print(\"X shape\",Xdata.shape,\"Y shape\",Ydata.shape,\"m\",m,\"n\",n,\"#Classes\",t)\n",
    "print(\"Classes:\",targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi manual Deep Neural Network with tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial cost:  2638.95\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "n_x = Xdata.shape[0]\n",
    "n_y = Ydata.shape[0]\n",
    "\n",
    "layer_dims = [n_x,2,2,n_y]\n",
    "nn_size = len(layer_dims)-1\n",
    "\n",
    "hidden_layer_activation=\"sigmoid\"\n",
    "layer_activations = []\n",
    "for i in range(1,nn_size):\n",
    "    layer_activations.append(hidden_layer_activation)\n",
    "layer_activations.append(\"sigmoid\")\n",
    "\n",
    "params = {}\n",
    "activations = {}\n",
    "\n",
    "diff_params = {}\n",
    " \n",
    "def initialize_params(n_dims):\n",
    "    \n",
    "    tmp_params = {}\n",
    "    \n",
    "    rows = tf.placeholder(tf.int32)\n",
    "    cols = tf.placeholder(tf.int32)\n",
    "    rand_op = tf.random_normal((rows,cols),mean=0.5, stddev=0.1,seed=1)\n",
    "    rand_b = tf.zeros((rows,cols))\n",
    "\n",
    "    with tf.Session() as s:\n",
    "        s.run(init_op)\n",
    "        for ls in range(1,len(n_dims)):\n",
    "            #print(n_dims[ls-1],n_dims[ls])\n",
    "            \n",
    "            tmp_params[\"W\"+str(ls)] = s.run(rand_op,feed_dict={rows:n_dims[ls],cols:n_dims[ls-1]})\n",
    "            tmp_params[\"b\"+str(ls)] = s.run(rand_b,feed_dict={rows:n_dims[ls],cols:1})#mean=0.0, stddev=1.0\n",
    "\n",
    "    return tmp_params\n",
    "\n",
    "def linear_activation(iX,iW,ib):\n",
    "    \n",
    "    Z = []\n",
    "    \n",
    "    W = tf.placeholder(tf.float32)\n",
    "    b = tf.placeholder(tf.float32)\n",
    "    X = tf.placeholder(tf.float32)\n",
    "    z = tf.add(tf.matmul(W,X),b)\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        Z = s.run(z,feed_dict={X:iX,W:iW,b:ib})\n",
    "    return Z\n",
    "def sigmoid_act(iX,iEpsilon=1e-7):\n",
    "    X = tf.placeholder(tf.float32)\n",
    "    epsilon=tf.placeholder(tf.float32)\n",
    "    sigmoid = tf.divide(1.0,tf.add(tf.add(1.0,tf.exp(-X)),epsilon))\n",
    "    with tf.Session() as s:\n",
    "        sg = s.run(sigmoid,feed_dict={X:iX,epsilon:iEpsilon})\n",
    "    return sg\n",
    "\n",
    "def activate(iZ,act_type=\"sigmoid\"):\n",
    "    tmp_act = []\n",
    "    \n",
    "    z = tf.placeholder(tf.float32)\n",
    "    sigmoid = tf.nn.sigmoid(z)\n",
    "    tanh = tf.nn.tanh(z)\n",
    "    relu = tf.nn.relu(z)\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        if act_type==\"sigmoid\":\n",
    "            tmp_act = sigmoid_act(iZ)\n",
    "        elif act_type==\"tanh\":\n",
    "            tmp_act = s.run(tanh,feed_dict={z:iZ})\n",
    "        elif act_type==\"relu\":\n",
    "            tmp_act = s.run(relu,feed_dict={z:iZ})\n",
    "    return tmp_act\n",
    "\n",
    "def forward_propagation(iX,iy,iparams,ilayer_activations,norm=False):\n",
    "    \n",
    "    tmp_activations = {\"A0\":iX}\n",
    "    \n",
    "    for i,act in enumerate(ilayer_activations):\n",
    "        z=linear_activation(tmp_activations[\"A\"+str(i)],iW=iparams[\"W\"+str(i+1)],ib=iparams[\"b\"+str(i+1)])\n",
    "        tmp_activations[\"A\"+str(i+1)]=activate(z,act)\n",
    "    return tmp_activations\n",
    "def calc_cost(iAL,iY):\n",
    "    im = float(iY.shape[0])\n",
    "    AL = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.float32)\n",
    "    m = tf.placeholder(tf.float32)\n",
    "\n",
    "    cost = -tf.reduce_mean(tf.reduce_sum(tf.add(tf.multiply(y,tf.log(AL)),tf.multiply(tf.subtract(1.0,y),tf.log(tf.subtract(1.0,AL)))),1))\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        tmp_cost = s.run(cost,feed_dict={AL:iAL,y:iY,m:im})\n",
    "    return tmp_cost\n",
    "def derivatives(iActivation,act_type=\"sigmoid\"):\n",
    "    \n",
    "    act = tf.placeholder(tf.float32)\n",
    "    sig_df = tf.multiply(act,tf.subtract(1.0,act))\n",
    "    tanh_df = tf.subtract(1.0,tf.pow(act,2))\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        if act_type==\"sigmoid\":\n",
    "            dev = s.run(sig_df,feed_dict={act:iActivation})\n",
    "        elif act_type==\"tanh\":\n",
    "            dev = s.run(tanh_df,feed_dict={act:iActivation})\n",
    "        elif act_type==\"relu\":\n",
    "            dev = (iActivation>0).astype(int).astype(float)\n",
    "    return dev\n",
    "\n",
    "def back_propagation(iActivations,iparams,iY,nn_size,activations):\n",
    "    iAl = iActivations[\"A\"+str(nn_size)]\n",
    "    im = float(iAl.shape[1])\n",
    "    \n",
    "    tmp_dev = {}\n",
    "    AL = tf.placeholder(tf.float32)\n",
    "    Y = tf.placeholder(tf.float32)\n",
    "    dJ_dAL = -tf.subtract(tf.divide(Y,AL),tf.divide(tf.subtract(1.0,Y),tf.subtract(1.0,AL)))\n",
    "    \n",
    "    dJ_dA_prev = tf.placeholder(tf.float32)\n",
    "    dA_dZ = tf.placeholder(tf.float32)\n",
    "    dJ_dZ = tf.multiply(dJ_dA_prev,dA_dZ)\n",
    "    \n",
    "    dJ_dZ2 = tf.placeholder(tf.float32)\n",
    "    dZ_dA = tf.placeholder(tf.float32)\n",
    "    dJ_dA = tf.matmul(tf.transpose(dZ_dA),dJ_dZ2)\n",
    "    \n",
    "    Act_prev = tf.placeholder(tf.float32)\n",
    "    m=tf.placeholder(tf.float32)\n",
    "    dJ_dW = tf.divide(tf.matmul(dJ_dZ2,tf.transpose(Act_prev)),m)\n",
    "    dJ_db = tf.reduce_mean(dJ_dZ2,1,keep_dims=True)\n",
    "    \n",
    "    with tf.Session() as s:\n",
    "        prev_dv = s.run(dJ_dAL,feed_dict={AL:iAl,Y:iY})\n",
    "        li=nn_size\n",
    "        while (li>0):\n",
    "            da_dz = derivatives(iActivations[\"A\"+str(li)],activations[li-1])\n",
    "            dJdZ = s.run(dJ_dZ,feed_dict={dJ_dA_prev:prev_dv,dA_dZ:da_dz})\n",
    "            dJdb = s.run(dJ_db,feed_dict={dJ_dZ2:dJdZ})\n",
    "            dJdW = s.run(dJ_dW,feed_dict={dJ_dZ2:dJdZ,Act_prev:iActivations[\"A\"+str(li-1)],m:im})\n",
    "            \n",
    "            tmp_dev[\"dW\"+str(li)]=dJdW\n",
    "            tmp_dev[\"db\"+str(li)]=dJdb\n",
    "            \n",
    "            prev_dv = s.run(dJ_dA,feed_dict={dJ_dZ2:dJdZ,dZ_dA:params[\"W\"+str(li)]})\n",
    "            li-=1\n",
    "    return tmp_dev\n",
    "def update_params(iParams,iDiff_params,learning_rate=0.1):\n",
    "    tmp_dic = {}\n",
    "    nl = len(iParams.keys())//2\n",
    "    W = tf.placeholder(tf.float32)\n",
    "    dW = tf.placeholder(tf.float32)\n",
    "    b = tf.placeholder(tf.float32)\n",
    "    db = tf.placeholder(tf.float32)\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "      \n",
    "    uW = tf.subtract(W,tf.multiply(dW,lr))\n",
    "    ub = tf.subtract(b,tf.multiply(db,lr))\n",
    "    with tf.Session() as s:\n",
    "        while nl>0:\n",
    "            tmp_dic[\"W\"+str(nl)]=s.run(uW,feed_dict={W:iParams[\"W\"+str(nl)],dW:iDiff_params[\"dW\"+str(nl)],lr:learning_rate})\n",
    "            tmp_dic[\"b\"+str(nl)]=s.run(ub,feed_dict={b:iParams[\"b\"+str(nl)],db:iDiff_params[\"db\"+str(nl)],lr:learning_rate})\n",
    "            nl-=1\n",
    "    return tmp_dic\n",
    "def norm_activation (iAct,axis=1,keep_dims=True):\n",
    "    act = tf.placeholder(tf.float32)\n",
    "    mean = tf.reduce_mean(act,axis=axis,keep_dims=keep_dims)\n",
    "    std = tf.pow(tf.reduce_mean(tf.pow(tf.subtract(act,mean),2),axis=axis,keep_dims=keep_dims),0.5)\n",
    "    norm = tf.divide(tf.subtract(act,mean),std)\n",
    "    with tf.Session() as s:\n",
    "        norm_act = s.run(norm,feed_dict={act:iAct})\n",
    "    return norm_act\n",
    "\n",
    "params = initialize_params(n_dims=layer_dims)\n",
    "activations=forward_propagation(iX=Xdata,iy=Ydata,iparams=params,ilayer_activations=layer_activations)\n",
    "last_layer=\"A\"+str(nn_size)\n",
    "\n",
    "act_l = activations[last_layer]\n",
    "\n",
    "cost = calc_cost(iAL=activations[\"A\"+str(nn_size)],iY=Ydata)\n",
    "print(\"Initial cost: \",cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using these activations: ['relu', 'relu', 'sigmoid']\n",
      "Layer size 3\n",
      "Initial cost:  1883.59\n",
      "Iter:  0 cost:  1201.19\n",
      "Iter:  1 cost:  923.079\n",
      "Iter:  2 cost:  754.138\n",
      "Iter:  3 cost:  644.712\n",
      "Iter:  4 cost:  569.77\n",
      "Iter:  5 cost:  516.065\n",
      "Iter:  6 cost:  476.149\n",
      "Iter:  7 cost:  445.594\n",
      "Iter:  8 cost:  421.631\n",
      "Iter:  9 cost:  402.457\n",
      "End loop\n",
      "It took:  288.6447114944458 seconds\n"
     ]
    }
   ],
   "source": [
    "n_x = Xdata.shape[0]\n",
    "n_y = Ydata.shape[0]\n",
    "\n",
    "layer_dims = [n_x,3,2,n_y]\n",
    "nn_size = len(layer_dims)-1\n",
    "\n",
    "iter_num=10\n",
    "learning_rate=1\n",
    "\n",
    "hidden_layer_activation=\"relu\"\n",
    "params = {}\n",
    "activations = {}\n",
    "diff_params = {}\n",
    "layer_activations = []\n",
    "for i in range(1,nn_size):\n",
    "    layer_activations.append(hidden_layer_activation)\n",
    "layer_activations.append(\"sigmoid\")\n",
    "\n",
    "print(\"Using these activations:\",layer_activations)\n",
    "print(\"Layer size\",nn_size)\n",
    "\n",
    "params = initialize_params(n_dims=layer_dims)\n",
    "time_one_s = time.time()\n",
    "activations=forward_propagation(iX=Xdata,iy=Ydata,iparams=params,ilayer_activations=layer_activations,norm=iNorm)\n",
    "last_layer=\"A\"+str(nn_size)\n",
    "\n",
    "act_l = activations[last_layer]\n",
    "\n",
    "cost = calc_cost(iAL=activations[\"A\"+str(nn_size)],iY=Ydata)\n",
    "print(\"Initial cost: \",cost)\n",
    "\n",
    "for _ in range(0,iter_num):\n",
    "    diff_params = back_propagation(iActivations=activations,iparams=params,iY=Ydata,nn_size=nn_size,activations=layer_activations)\n",
    "    params = update_params(params,diff_params,learning_rate=learning_rate)\n",
    "    activations=forward_propagation(iX=Xdata,iy=Ydata,iparams=params,ilayer_activations=layer_activations)\n",
    "    cost = calc_cost(iAL=activations[\"A\"+str(nn_size)],iY=Ydata)\n",
    "    print(\"Iter: \",_,\"cost: \",cost)\n",
    "print(\"End loop\")\n",
    "time_one_e = time.time()\n",
    "print(\"It took: \",str(time_one_e-time_one_s),\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More pure tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural network of hidden layers sizes 5,4,3,2 with activation function ReLU and output layer sigmoid. We don't use the sigmoid function from tensorflow because without modification the last layer produces **1.0** and this result in a logistic cost function = log(1.0)= 0. Thus, if **y==0** we will get a **Nan** value. To solve this we add an epsilon value on the denominator so it won't reach 1.0. You can see which values of **Z** (input of sigmoid) along values of epsilon will cause a sigmoid==1.0, thus Nan in the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2376, 2500) (2376, 38)\n",
      "32071.3\n",
      "21492.2\n",
      "339.918\n",
      "295.292\n",
      "290.841\n",
      "287.65\n",
      "285.282\n",
      "283.522\n",
      "282.221\n",
      "281.279\n",
      "280.605\n",
      "280.123\n",
      "279.771\n",
      "279.507\n",
      "279.306\n",
      "279.143\n",
      "279.015\n",
      "278.908\n",
      "278.818\n",
      "278.744\n",
      "278.678\n",
      "278.628\n",
      "278.581\n",
      "278.543\n",
      "278.512\n",
      "278.483\n",
      "278.46\n",
      "278.442\n",
      "278.424\n",
      "278.409\n",
      "278.397\n",
      "278.388\n",
      "278.378\n",
      "278.371\n",
      "278.365\n",
      "278.359\n",
      "278.356\n",
      "278.352\n",
      "278.348\n",
      "278.345\n",
      "278.345\n",
      "Convergence reached at iter: 39\n",
      "Accuracy:  14400.0 %\n",
      "End loop\n",
      "Time taken was:  3.0 Seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFM1JREFUeJzt3X+MZWV9x/H3d+7cu3NBQX5MN7hgF+P+USAVw4aQaBtb\nUt2qLdioWZPWbUKlCWg1bdJK21T7B4k2/mhNKwmKcaEqEn8EYqQG0cT4h+CgyC+lbAUK25VdRUVc\ndtnZ+faP+9yZM/fXDDOzey+e9yu5mTPPPefe733Y3Q/P85xzT2QmkiRVTY27AEnS5DEcJEl9DAdJ\nUh/DQZLUx3CQJPUxHCRJfQwHSVIfw0GS1MdwkCT1mR53AWt1+umn59atW8ddhiQ9r9x1110/yczZ\nlfZ73obD1q1bmZubG3cZkvS8EhGPrmY/p5UkSX0MB0lSH8NBktTHcJAk9TEcJEl9DAdJUh/DQZLU\np3bh8J1HnuSDX32QowveHlWShqldONz9vz/n37+xh2eOHB13KZI0sWoXDjOtBgDPPGs4SNIwtQuH\nE5qdcDjkyEGShqpdOLS7IwfDQZKGql84lJHDQaeVJGmo2oXDTNM1B0laSe3CoTut5JqDJA1Xv3Bo\nuuYgSSupbzg4rSRJQ9UvHDxbSZJWtGI4RMRZEfGNiHggIu6PiHeV9lMj4raIeKj8PKVyzFURsSci\nHoyI11baL4iIe8tzH42IKO2bIuJzpf2OiNi68R+1wzUHSVrZakYO88DfZOY5wEXAlRFxDvAe4PbM\n3AbcXn6nPLcTOBfYAXwsIhrlta4B3g5sK48dpf0y4GeZ+TLgI8AHNuCzDTQz3fnInsoqScOtGA6Z\nuS8zv1u2fwn8ANgCXALsLrvtBi4t25cAN2bm4cx8GNgDXBgRZwAnZea3MzOB63uO6b7W54GLu6OK\njTbdmKLVmHJaSZJGeE5rDmW65xXAHcDmzNxXnvoxsLlsbwEeqxz2eGnbUrZ725cdk5nzwC+A055L\nbc/FTHPKBWlJGmHV4RARLwC+ALw7M5+qPldGAsf8O7Aj4vKImIuIuQMHDqz5ddqthmsOkjTCqsIh\nIpp0guHTmfnF0vxEmSqi/Nxf2vcCZ1UOP7O07S3bve3LjomIaeBk4Ke9dWTmtZm5PTO3z87Orqb0\ngdrNhtNKkjTCas5WCuA64AeZ+eHKU7cAu8r2LuDmSvvOcgbS2XQWnu8sU1BPRcRF5TXf1nNM97Xe\nBHy9jEaOiZlmw2klSRphehX7vBL4M+DeiLi7tP098H7gpoi4DHgUeAtAZt4fETcBD9A50+nKzOz+\nS3wF8CmgDdxaHtAJnxsiYg/wJJ2znY6ZE1qOHCRplBXDITO/BQw7c+jiIcdcDVw9oH0OOG9A+yHg\nzSvVslHaLUcOkjRK7a6QBtccJGkltQyHGcNBkkaqZTi0mw0OOa0kSUPVMxxckJakkeoZDk4rSdJI\n9QyHVoNDRxZYWDjmF3VL0vNSPcOh3PDn0LyjB0kapJ7h0PJucJI0Si3DYcb7SEvSSLUMh8VpJcNB\nkgaqdTg88+zCmCuRpMlUz3BoOa0kSaPUMhxcc5Ck0WoZDicsnq00P+ZKJGky1TIc2o4cJGmkeoZD\nywVpSRqlluHgmoMkjVbLcPA6B0karZbh0GwEjanw6zMkaYhahkNE+LXdkjRCLcMBvOGPJI1S33Bo\nNpxWkqQhDAdJUp/ahsOM00qSNFRtw6HdnDIcJGmIGodDw+scJGmI+oZDyzUHSRqmtuEw43UOkjRU\nbcPhBEcOkjRUbcPBK6Qlabjah0NmjrsUSZo4tQ2HmVaDTDg87z0dJKlXbcPBr+2WpOFqHw6uO0hS\nv/qGw+KtQg0HSepV33AoI4eDhoMk9alvOLRcc5CkYeobDq45SNJQK4ZDRHwyIvZHxH2VtvdFxN6I\nuLs8Xld57qqI2BMRD0bEayvtF0TEveW5j0ZElPZNEfG50n5HRGzd2I842EzTNQdJGmY1I4dPATsG\ntH8kM88vj68ARMQ5wE7g3HLMxyKiUfa/Bng7sK08uq95GfCzzHwZ8BHgA2v8LM/J4oK0IwdJ6rNi\nOGTmN4EnV/l6lwA3ZubhzHwY2ANcGBFnACdl5rezc0ny9cCllWN2l+3PAxd3RxXHktc5SNJw61lz\neGdE3FOmnU4pbVuAxyr7PF7atpTt3vZlx2TmPPAL4LR11LUqbaeVJGmotYbDNcBLgfOBfcCHNqyi\nESLi8oiYi4i5AwcOrOu1utNKBx05SFKfNYVDZj6RmUczcwH4OHBheWovcFZl1zNL296y3du+7JiI\nmAZOBn465H2vzcztmbl9dnZ2LaUv2jQ9RQQccuQgSX3WFA5lDaHrjUD3TKZbgJ3lDKSz6Sw835mZ\n+4CnIuKisp7wNuDmyjG7yvabgK/ncfiq1Ijwa7slaYjplXaIiM8CrwZOj4jHgfcCr46I84EEHgH+\nEiAz74+Im4AHgHngyszs/ut7BZ0zn9rAreUBcB1wQ0TsobPwvXMjPthqGA6SNNiK4ZCZbx3QfN2I\n/a8Grh7QPgecN6D9EPDmleo4FmaaDZ551q/slqRetb1CGjqL0p7KKkn96h0OTitJ0kC1D4eDz86P\nuwxJmjj1DodWg2eOuOYgSb3qHQ7Nhtc5SNIA9Q6HlmsOkjRIrcNhxgVpSRqo1uHgtJIkDVbvcGhN\nOXKQpAHqHQ7NBvMLyZGjnrEkSVX1DodW59tDDjq1JEnL1DscvBucJA1U73BodT6+d4OTpOXqHQ7d\nW4U6cpCkZWodDjOGgyQNVOtwWFxzcFpJkpapdzi0HDlI0iD1DocycvBUVklart7h4MhBkgaqdzh4\nnYMkDVTvcOiOHJxWkqRlah0OM9NOK0nSILUOh6mpYNO038wqSb1qHQ7QmVryOgdJWs5waDY8lVWS\nehgO3kdakvoYDs2Gp7JKUg/DoenIQZJ6GQ6thtc5SFKP2ofDTLPBM0e8h7QkVdU+HFxzkKR+hkOz\nwcFn58ddhiRNFMPBNQdJ6mM4tBoccs1BkpYxHJoNnj26wPxRA0KSugyH7j0d5g0HSeqqfTjMeE8H\nSepT+3DwbnCS1G/FcIiIT0bE/oi4r9J2akTcFhEPlZ+nVJ67KiL2RMSDEfHaSvsFEXFvee6jERGl\nfVNEfK603xERWzf2I47WDQe/QkOSlqxm5PApYEdP23uA2zNzG3B7+Z2IOAfYCZxbjvlYRDTKMdcA\nbwe2lUf3NS8DfpaZLwM+AnxgrR9mLdqtThf4td2StGTFcMjMbwJP9jRfAuwu27uBSyvtN2bm4cx8\nGNgDXBgRZwAnZea3MzOB63uO6b7W54GLu6OK46HdnAZcc5CkqrWuOWzOzH1l+8fA5rK9BXisst/j\npW1L2e5tX3ZMZs4DvwBOG/SmEXF5RMxFxNyBAwfWWPpy7ZZrDpLUa90L0mUkkBtQy2re69rM3J6Z\n22dnZzfkNV1zkKR+aw2HJ8pUEeXn/tK+Fzirst+ZpW1v2e5tX3ZMREwDJwM/XWNdz9liODitJEmL\n1hoOtwC7yvYu4OZK+85yBtLZdBae7yxTUE9FxEVlPeFtPcd0X+tNwNfLaOS4mCkL0o4cJGnJ9Eo7\nRMRngVcDp0fE48B7gfcDN0XEZcCjwFsAMvP+iLgJeACYB67MzO6/ulfQOfOpDdxaHgDXATdExB46\nC987N+STrZLXOUhSvxXDITPfOuSpi4fsfzVw9YD2OeC8Ae2HgDevVMexMlPCwVNZJWlJ7a+Qbjam\naDbCaSVJqqh9OEBnaskFaUlaYjjQvaeD4SBJXYYDZeRgOEjSIsOBzqK000qStMRwoNxH2pGDJC0y\nHHBBWpJ6GQ645iBJvQwHnFaSpF6GA52RwyGnlSRpkeGAIwdJ6mU44JqDJPUyHOhc53DoyAILC8ft\nm8IlaaIZDlRuFTrv6EGSwHAAvBucJPUyHPA+0pLUy3CgMq1kOEgSYDgA1WmlhTFXIkmTwXBgaeTg\ntJIkdRgOLN1H2nCQpA7Dgeq00vyYK5GkyWA44LSSJPUyHHBBWpJ6GQ44cpCkXoYDSyMHr3OQpA7D\nAWg2gsZU+PUZklQYDkBE+LXdklRhOBQzhoMkLTIcinZrymklSSoMh6LdbBgOklQYDkW7Ne20kiQV\nhkPRbk4ZDpJUGA5Fu9nwOgdJKgyHot1yzUGSugyHwlNZJWmJ4VB4tpIkLTEcCq+QlqQl6wqHiHgk\nIu6NiLsjYq60nRoRt0XEQ+XnKZX9r4qIPRHxYES8ttJ+QXmdPRHx0YiI9dS1Fu1WJxwy83i/tSRN\nnI0YOfxeZp6fmdvL7+8Bbs/MbcDt5Xci4hxgJ3AusAP4WEQ0yjHXAG8HtpXHjg2o6zlptxpkwuF5\n7+kgScdiWukSYHfZ3g1cWmm/MTMPZ+bDwB7gwog4AzgpM7+dnf9tv75yzHHj13ZL0pL1hkMCX4uI\nuyLi8tK2OTP3le0fA5vL9hbgscqxj5e2LWW7t/24WrwbnOEgSUyv8/hXZebeiPgN4LaI+GH1yczM\niNiwSfwSQJcDvOQlL9molwUqd4PzjCVJWt/IITP3lp/7gS8BFwJPlKkiys/9Zfe9wFmVw88sbXvL\ndm/7oPe7NjO3Z+b22dnZ9ZTeZ6aMHA4aDpK09nCIiBMj4oXdbeA1wH3ALcCustsu4OayfQuwMyI2\nRcTZdBae7yxTUE9FxEXlLKW3VY45blxzkKQl65lW2gx8qZx1Og18JjP/KyK+A9wUEZcBjwJvAcjM\n+yPiJuABYB64MjO7/xJfAXwKaAO3lsdxtTitZDhI0trDITN/BLx8QPtPgYuHHHM1cPWA9jngvLXW\nshEWF6SdVpIkr5DucuQgSUsMh8I1B0laYjgUTitJ0hLDoehOKx105CBJhkPXpulOVxxy5CBJhkNX\nRPi13ZJUGA4V3a/tlqS6MxwqOneD8yu7JclwqGi3Gp7KKkkYDsu45iBJHYZDRbvZ4OCz8+MuQ5LG\nznComGk1eOaIaw6SZDhUtJtTXucgSRgOy7jmIEkdhkNFuzVtOEgShsMy7WbDaSVJwnBYpt2acuQg\nSRgOy7SbDeYXkiNHPWNJUr0ZDhUz5Z4OB51aklRzhkNF954OfoWGpLozHCq8G5wkdRgOFYvh4MhB\nUs0ZDhXdaSXDQVLdGQ4V3ZGD1zpIqjvDocKRgyR1GA4VbU9llSTAcFhmxgVpSQIMh2W8zkGSOgyH\nCq9zkKQOw6HC6xwkqcNwqJiaCjZN+82skmQ49Gi3vKeDJBkOPdrNhqeySqo9w6GH95GWJMOhz0yz\n4amskmrPcOjRbjlykCTDoUe72fA6B0m1Nz3uAiZNu9Xg4Z/8ii/f83+cuGmaF26a5gUz05zYmuaF\nM9OcuGmaZsNMlfTrbWLCISJ2AP8GNIBPZOb7x1HHWaecwG0PPME7PvO9oftEQABTEZ3tCKK0Ty1u\nd352DmCprbJdnqKzGctem8V2iMpzS3suvUZvbQO3e15j6Gcb/fTA91z18Su9+Ervvb7D1/feK3Wc\nxqKu/1X+6uJt/NHLX3xM32MiwiEiGsB/AH8APA58JyJuycwHjnct//j63+IvfudsfnV4nl8enufp\nQ/PLtp8+PM+RowtkQpIsJJ3tTBJYWOj8zOy8XpJL2zn4ufJrac/K9vJ9l/++pHef3h2WXr96VL/R\nzy69z1qOX+m9V7K+o9dprG+uYbLG/2FObjeP+XtMRDgAFwJ7MvNHABFxI3AJcNzDYWoqePGL2sf7\nbSVpokzK5PkW4LHK74+XNknSGExKOKxKRFweEXMRMXfgwIFxlyNJv7YmJRz2AmdVfj+ztC2Tmddm\n5vbM3D47O3vcipOkupmUcPgOsC0izo6IFrATuGXMNUlSbU3EgnRmzkfEO4Cv0jmV9ZOZef+Yy5Kk\n2pqIcADIzK8AXxl3HZKkyZlWkiRNEMNBktQn1nvl6rhExAHg0TUefjrwkw0sZyNZ29pY29pY29o8\nn2v7zcxc8XTP5204rEdEzGXm9nHXMYi1rY21rY21rU0danNaSZLUx3CQJPWpazhcO+4CRrC2tbG2\ntbG2tfm1r62Waw6SpNHqOnKQJI1Qu3CIiB0R8WBE7ImI94y7nqqIeCQi7o2IuyNibsy1fDIi9kfE\nfZW2UyPitoh4qPw8ZYJqe19E7C19d3dEvG5MtZ0VEd+IiAci4v6IeFdpH3vfjaht7H0XETMRcWdE\nfL/U9s+lfRL6bVhtY++3So2NiPheRHy5/L7ufqvVtFK549x/U7njHPDWcdxxbpCIeATYnpljP386\nIn4XeBq4PjPPK23/AjyZme8vwXpKZv7dhNT2PuDpzPzg8a6np7YzgDMy87sR8ULgLuBS4M8Zc9+N\nqO0tjLnvonMf1hMz8+mIaALfAt4F/Anj77dhte1gAv7MAUTEXwPbgZMy8w0b8Xe1biOHxTvOZeaz\nQPeOc+qRmd8EnuxpvgTYXbZ30/mH5bgbUttEyMx9mfndsv1L4Ad0blw19r4bUdvYZcfT5ddmeSST\n0W/DapsIEXEm8HrgE5Xmdfdb3cJh0u84l8DXIuKuiLh83MUMsDkz95XtHwObx1nMAO+MiHvKtNNY\npryqImIr8ArgDias73pqgwnouzI1cjewH7gtMyem34bUBhPQb8C/An8LLFTa1t1vdQuHSfeqzDwf\n+EPgyjJ9MpGyMx85Mf/3BFwDvBQ4H9gHfGicxUTEC4AvAO/OzKeqz4277wbUNhF9l5lHy5//M4EL\nI+K8nufH1m9Daht7v0XEG4D9mXnXsH3W2m91C4dV3XFuXDJzb/m5H/gSnWmwSfJEmbfuzl/vH3M9\nizLzifIXeAH4OGPsuzIv/QXg05n5xdI8EX03qLZJ6rtSz8+Bb9CZ05+IfhtU24T02yuBPy7rlTcC\nvx8R/8kG9FvdwmFi7zgXESeWRUIi4kTgNcB9o4867m4BdpXtXcDNY6xlme5fhOKNjKnvyuLldcAP\nMvPDlafG3nfDapuEvouI2Yh4Udlu0zlp5IdMRr8NrG0S+i0zr8rMMzNzK51/z76emX/KRvRbZtbq\nAbyOzhlL/wP8w7jrqdT1UuD75XH/uGsDPktnqHyEztrMZcBpwO3AQ8DXgFMnqLYbgHuBe8pfjDPG\nVNur6Azh7wHuLo/XTULfjaht7H0H/DbwvVLDfcA/lfZJ6LdhtY2933rqfDXw5Y3qt1qdyipJWp26\nTStJklbBcJAk9TEcJEl9DAdJUh/DQZLUx3CQJPUxHCRJfQwHSVKf/wdhmndQsyswFgAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f647ba27278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ydata = Ydata.astype(\"float32\")\n",
    "\n",
    "print(Xdata.shape,Ydata.shape)\n",
    "\n",
    "n_x = Xdata.shape[1]\n",
    "m = Xdata.shape[1]\n",
    "n_y = Ydata.shape[1]\n",
    "layer_dims = [n_x,3,2,n_y]\n",
    "\n",
    "learning_rate= tf.constant(0.1)\n",
    "\n",
    "hl_1 = 5\n",
    "hl_2 = 4\n",
    "hl_3 = 3\n",
    "hl_4 = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal((n_x,hl_1),mean=0.5, stddev=0.1,seed=1))\n",
    "b1 = tf.Variable(tf.zeros((1,hl_1)))\n",
    "Z1 = tf.add(tf.matmul(X,W1),b1)\n",
    "A1 = tf.nn.relu(Z1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal((hl_1,hl_2),mean=0.5, stddev=0.1,seed=1))\n",
    "b2 = tf.Variable(tf.zeros((1,hl_2)))\n",
    "Z2 = tf.add(tf.matmul(A1,W2),b2)\n",
    "A2 = tf.nn.relu(Z2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal((hl_2,hl_3),mean=0.5, stddev=0.1,seed=1))\n",
    "b3 = tf.Variable(tf.zeros((1,hl_3)))\n",
    "Z3 = tf.add(tf.matmul(A2,W3),b3)\n",
    "A3 = tf.nn.relu(Z3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal((hl_3,hl_4),mean=0.5, stddev=0.1,seed=1))\n",
    "b4 = tf.Variable(tf.zeros((1,hl_4)))\n",
    "Z4 = tf.add(tf.matmul(A3,W4),b4)\n",
    "A4 = tf.nn.relu(Z4)\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal((hl_4,n_y),mean=0.5, stddev=0.1,seed=1))\n",
    "b5 = tf.Variable(tf.zeros((1,n_y)))\n",
    "ZL = tf.add(tf.matmul(A4,W5),b5)\n",
    "\n",
    "epsilon = tf.constant(1e-6)\n",
    "AL = tf.divide(1.0,tf.add(tf.add(1.0,tf.exp(-ZL)),epsilon))\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(AL,1))\n",
    "accuracy = tf.multiply(tf.reduce_mean(tf.cast(correct_prediction,tf.float32)),100)\n",
    "\n",
    "#A3 = tf.nn.sigmoid(Z3) \n",
    "\n",
    "logistic_cost_function = -tf.reduce_mean(tf.reduce_sum(tf.add(tf.multiply(y,tf.log(AL)),tf.multiply(tf.subtract(1.0,y),tf.log(tf.subtract(1.0,AL)))),0))\n",
    "training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(logistic_cost_function)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "iters = 40\n",
    "params = []\n",
    "init_cost = 0\n",
    "cost_diff_margin = 1e-6\n",
    "costs = []\n",
    "costs_diff = []\n",
    "\n",
    "time_two_s = time.time()\n",
    "with tf.Session() as s:\n",
    "    s.run(init_op)\n",
    "        \n",
    "    a3,_,cost = s.run([A3,training_step,logistic_cost_function],feed_dict={X:Xdata,y:Ydata})\n",
    "    init_cost = cost\n",
    "    prev_cost = init_cost\n",
    "    print(cost)\n",
    "    \n",
    "    for iterx in range(0,iters):\n",
    "        _,cost = s.run([training_step,logistic_cost_function],feed_dict={X:Xdata,y:Ydata})\n",
    "        print(cost)\n",
    "        costs.append(cost)\n",
    "        diff_cost = prev_cost - cost\n",
    "        costs_diff.append(diff_cost)\n",
    "        prev_cost = cost\n",
    "        if diff_cost < cost_diff_margin:\n",
    "            print(\"Convergence reached at iter:\",iterx)\n",
    "            break;\n",
    "    params = s.run([W1,b1,W2,b2,W3,b3])\n",
    "    pred = s.run(accuracy,feed_dict={X:Xdata,y:Ydata})\n",
    "    print(\"Accuracy: \",pred,\"%\")\n",
    "print(\"End loop\")\n",
    "time_two_e = time.time()\n",
    "time_taken = time_two_e -time_two_s\n",
    "print(\"Time taken was: \",str(np.round(time_taken)),\"Seconds\")\n",
    "\n",
    "plt.plot(np.arange(0,len(costs)),costs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I show which values of Z (Input of sigmoid) along with values of epsilon causes the sigmoid function to produce a 1.0, thus a Nan in the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid is 1 when Z=10^2.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^2.0 and epsilon is e^-9\n",
      "Sigmoid is 1 when Z=10^3.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^3.0 and epsilon is e^-9\n",
      "Sigmoid is 1 when Z=10^4.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^4.0 and epsilon is e^-9\n",
      "Sigmoid is 1 when Z=10^5.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^5.0 and epsilon is e^-9\n",
      "Sigmoid is 1 when Z=10^6.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^6.0 and epsilon is e^-9\n",
      "Sigmoid is 1 when Z=10^7.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^7.0 and epsilon is e^-9\n",
      "Sigmoid is 1 when Z=10^8.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^8.0 and epsilon is e^-9\n",
      "Sigmoid is 1 when Z=10^9.0 and epsilon is e^-8\n",
      "Sigmoid is 1 when Z=10^9.0 and epsilon is e^-9\n",
      "[[ 0.90905333  0.99005449  0.99895561  0.99985456  0.99994457  0.99995363\n",
      "   0.99995446  0.99995458  0.99995458]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]\n",
      " [ 0.90909088  0.99009901  0.99900097  0.99989998  0.99998999  0.99999905\n",
      "   0.99999988  1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=tf.placeholder(tf.float32)\n",
    "p=tf.placeholder(tf.float32)\n",
    "xe = tf.pow(x,p)\n",
    "\n",
    "ep = tf.placeholder(tf.float32)\n",
    "epsilon=tf.pow(10.0,-ep)\n",
    "\n",
    "sigmoid = tf.divide(1.0,tf.add(epsilon,tf.add(1.0,tf.exp(-xe))))\n",
    "\n",
    "sigm = np.zeros((9,9))\n",
    "\n",
    "with tf.Session() as s:\n",
    "    for i in range(1,10):\n",
    "        for ei in range(1,10):\n",
    "            ix=float(10)\n",
    "            ip=float(i)\n",
    "            r1 = s.run(sigmoid,feed_dict={x:ix,p:ip,ep:ei})\n",
    "            if r1>=1:\n",
    "                print(\"Sigmoid is 1 when Z=10^\"+str(ip)+\" and epsilon is e^-\"+str(ei))\n",
    "            sigm[i-1,ei-1]=r1\n",
    "print(sigm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
